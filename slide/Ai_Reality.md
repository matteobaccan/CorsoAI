


bbc full body capture of Dwayne Johnson at Jumanji Welcome to the Jungle, smiling, bald, athletic, strong, sunglasses, he is dressed as a zoo tourist guide, at jungle outdoors. extreme detail, digital art, 4k, ultra hd, hyperrealism, trending on Instagram.



Impatto dell'IA sulla società (00:04 - 10:02)

I modelli di IA possono contribuire al cambiamento climatico. 

    I dati di addestramento utilizzano arte e libri senza consenso.
    L'implementazione può discriminare contro alcune comunità.

I grandi modelli linguistici hanno costi ambientali significativi.

    Bloom, un grande modello linguistico open source, ha emesso tonnellate di anidride carbonica durante l'addestramento.
    Passare a un modello linguistico più grande emette più anidride carbonica per lo stesso compito.

Strumenti per misurare e mitigare gli impatti dell'IA

CodeCarbon stima il consumo di energia e le emissioni di carbonio durante l'addestramento dell'IA.

    Possono essere fatte scelte informate per scegliere modelli più sostenibili.
    I modelli di IA possono essere implementati con energie rinnovabili per ridurre le emissioni.

Spawning.ai ha creato "Have I Been Trained?" per aiutare artisti e autori a tracciare l'uso delle loro opere. 

    Utilizzato come prova cruciale in una class action per violazione del copyright.
    In collaborazione con Hugging Face per creare meccanismi di opt-in e opt-out per i set di dati.

Affrontare i pregiudizi nell'IA

I pregiudizi nell'IA possono portare a rappresentazioni di stereotipi e discriminazioni.

    I sistemi di riconoscimento facciale hanno mostrato pregiudizi contro le donne di colore. 
    Modelli prevenuti implementati nelle forze dell'ordine possono portare ad accuse false e ingiusto imprigionamento.

Lo strumento Stable Bias Explorer esplora i pregiudizi nei modelli di generazione di immagini attraverso la lente delle professioni.

    Ha dimostrato una significativa rappresentazione di bianchezza e mascolinità nelle professioni.
    Presentato a un evento ONU sui pregiudizi di genere come esempio di comprensione dell'IA.

Importanza della trasparenza e degli strumenti

L'IA dovrebbe rimanere accessibile per comprenderne il funzionamento e i limiti.

    Gli strumenti per misurare l'impatto dell'IA possono informare le decisioni di aziende e legislatori.
    Gli utenti possono utilizzare le informazioni per scegliere modelli di IA affidabili.

Progresso dell'IA (10:04 - 10:16) 

"L'IA sta progredendo rapidamente, ma non è un dato di fatto."

    Stiamo costruendo la strada mentre la percorriamo
    Decisione collettiva sulla direzione

Decision Making collettivo

"Possiamo decidere collettivamente quale direzione prendere insieme."

    Importanza del decision making collettivo 
    Collaborazione per plasmare il futuro dell'IA


----------------

AI Is Dangerous, but Not for the Reasons You Think | Sasha Luccioni | TED

https://www.youtube.com/watch?v=eXdVDhOGqoE

Quindi sono stato un ricercatore di intelligenza artificiale per oltre un decennio. E un paio di mesi fa, ho ricevuto
l'email più strana della mia carriera. Uno sconosciuto mi ha scritto dicendo che il mio lavoro sull'AI metterà fine
all'umanità. Lo capisco, l'AI è così di moda in questo momento. (Risate) È sulla cresta dell'onda praticamente ogni
giorno, a volte per cose davvero interessanti come scoprire nuove molecole per la medicina o quel papa figo con il
piumino bianco. 

![center w:450px](img/ai-reality-papa-col-piumino.jpg)


Ma altre volte i titoli sono stati davvero cupi, come quel chatbot che ha detto a quel tizio che
dovrebbe divorziare da sua moglie o quell'app di pianificazione dei pasti AI che proponeva una ricetta accattivante a
base di gas cloro. E sullo sfondo, abbiamo sentito un sacco di discorsi su scenari apocalittici, rischi esistenziali e
la singolarità, con lettere scritte ed eventi organizzati per assicurarsi che non accada. Ora, sono un ricercatore che
studia gli impatti dell'AI sulla società, e non so cosa succederà tra 10 o 20 anni, e nessuno lo sa davvero. Ma quello
che so è che ci sono alcune cose abbastanza brutte che stanno accadendo ora, perché l'AI non esiste nel vuoto. Fa parte
della società e ha impatti sulle persone e sul pianeta. I modelli di AI possono contribuire al cambiamento climatico. I
loro dati di addestramento utilizzano arte e libri creati da artisti e autori senza il loro consenso. E il loro
dispiegamento può discriminare intere comunità. Ma dobbiamo iniziare a tracciarne gli impatti. Dobbiamo iniziare ad
essere trasparenti e rivelarli e creare strumenti in modo che le persone comprendano meglio l'AI, in modo che speriamo
le future generazioni di modelli di AI siano più affidabili, sostenibili, forse meno inclini a ucciderci, se è quello
che vi piace. Ma cominciamo con la sostenibilità, perché quella nuvola in cui vivono i modelli di AI è in realtà fatta
di metallo, plastica ed alimentata da enormi quantità di energia. E ogni volta che interroghi un modello di AI, comporta
un costo per il pianeta.

L'anno scorso, ho fatto parte dell'iniziativa BigScience, che ha riunito un migliaio di ricercatori da tutto il mondo
per creare Bloom, il primo grande modello di linguaggio aperto, come ChatGPT, ma con un'enfasi sull'etica, la
trasparenza e il consenso. E lo studio che ho guidato che ha esaminato gli impatti ambientali di Bloom ha scoperto che
solo l'addestramento ha utilizzato tanta energia quanta ne consumano le case in un intero anno ed ha emesso tonnellate
di anidride carbonica, che equivale a guidare la propria auto cinque volte intorno al pianeta solo perché qualcuno possa
usare questo modello per raccontare una barzelletta sulla porta. E questo potrebbe non sembrare molto, ma altri modelli
di linguaggio di grandi dimensioni simili, come GPT-3, emettono molte volte di più anidride carbonica. Ma il fatto è che
le aziende tecnologiche non misurano queste cose. Non le divulgano. E quindi questo è probabilmente solo la punta
dell'iceberg, anche se è un iceberg che si sta sciogliendo. E negli ultimi anni abbiamo visto i modelli di AI aumentare
di dimensioni perché la tendenza attuale nell'AI è "più grande è meglio". Ma per favore non fatemi iniziare sul perché
sia così. In ogni caso, abbiamo visto i grandi modelli di linguaggio in particolare crescere di 10.000 volte in
dimensioni negli ultimi cinque anni. E ovviamente, anche i loro costi ambientali stanno aumentando. Il lavoro più
recente che ho guidato ha scoperto che passare da un modello più piccolo ed efficiente a un modello di linguaggio più
grande emette 30 volte più carbonio per lo stesso compito. Come raccontare quella barzelletta sulla porta. E mentre
stiamo mettendo questi modelli nei cellulari, motori di ricerca, frigoriferi intelligenti e altoparlanti, i costi
ambientali si stanno accumulando rapidamente. Quindi, invece di concentrarci su alcuni rischi esistenziali futuri,
parliamo degli impatti tangibili attuali e degli strumenti che possiamo creare per misurare e mitigare questi impatti.
Ho contribuito a creare CodeCarbon, uno strumento che funziona in parallelo con il codice di addestramento di AI che
stima la quantità di energia che consuma e la quantità di carbonio che emette. E utilizzare uno strumento del genere può
aiutarci a fare scelte informate, come scegliere un modello piuttosto che un altro perché è più sostenibile, o schierare
modelli di AI con energie rinnovabili che possono drasticamente ridurre le loro emissioni. Ma parliamo di altre cose
perché ci sono altri impatti dell'AI
oltre alla sostenibilità. Per esempio, è stato davvero difficile per artisti e autori dimostrare che il lavoro della
loro vita è stato utilizzato per addestrare modelli di AI senza il loro consenso. E se vuoi fare causa a qualcuno, di
solito hai bisogno di prove, giusto? Quindi Spawning.ai, un'organizzazione fondata da artisti, ha creato questo
strumento davvero figo chiamato "Have I Been Trained?". Ti permette di cercare in questi enormi set di dati per vedere
cosa hanno su di te. Lo ammetto, ero curioso. Ho cercato in LAION-B, che è questo enorme set di dati di immagini e
testi, per vedere se c'erano immagini di me lì dentro. Ora quelle prime due immagini, sono io da eventi in cui ho
parlato. Ma le altre immagini, nessuna di quelle sono io. Sono probabilmente di altre donne di nome Sasha che hanno
messo le loro fotografie su internet. E questo può probabilmente spiegare perché, quando interrogo un modello di
generazione di immagini per generare una fotografia di una donna di nome Sasha, più spesso che no ottengo immagini di
modelle in bikini. A volte hanno due braccia, a volte ne hanno tre, ma raramente hanno vestiti addosso. E mentre può
essere interessante per persone come te e me cercare in questi set di dati, per artisti come Karla Ortiz, questo
fornisce prove cruciali che il lavoro della sua vita, la sua arte, è stato utilizzato per addestrare modelli di AI senza
il suo consenso, e lei e due artisti hanno usato questo come prova per intentare una causa collettiva contro le aziende
di AI per violazione del diritto d'autore. E più recentemente - (Applausi) E più recentemente Spawning.ai si è associata
a Hugging Face, l'azienda in cui lavoro, per creare meccanismi di opt-in e opt-out per la creazione di questi set di
dati. Perché l'arte creata dagli umani non dovrebbe essere un buffet tutto-quello-che-puoi-mangiare per addestrare
modelli di linguaggio AI. (Applausi) L'ultima cosa di cui voglio parlare è il pregiudizio. Ne avrete probabilmente
sentito parlare molto. Formalmente, si verifica quando i modelli di AI codificano schemi e convinzioni che possono
rappresentare stereotipi o razzismo e sessismo. Una delle mie eroine, la dott.ssa Joy Buolamwini, ha
sperimentato questo in prima persona quando si è resa conto che i sistemi di AI non rilevavano nemmeno il suo viso a
meno che non indossasse una maschera di colore bianco. Scavando più a fondo, ha scoperto che i comuni sistemi di
riconoscimento facciale erano molto peggiori per le donne di colore rispetto agli uomini bianchi. E quando modelli
distorti come questi vengono dispiegati in ambito delle forze dell'ordine, questo può portare ad accuse false, persino a
detenzioni ingiuste, che abbiamo visto accadere a più persone negli ultimi mesi. Per esempio, Porcha Woodruff è stata
ingiustamente accusata di rapina a mano armata all'ottavo mese di gravidanza perché un sistema di AI l'ha identificata
erroneamente. Ma purtroppo, questi sistemi sono scatole nere, e persino i loro creatori non possono dire esattamente
perché funzionano in quel modo. E per esempio, per i sistemi di generazione di immagini, se vengono usati in contesti
come la generazione di una scheda segnaletica forense basata sulla descrizione di un colpevole, essi prendono tutti quei
pregiudizi e li rispediscono indietro per termini come "criminale pericoloso", "terrorista" o "membro di una gang", cosa
ovviamente super pericolosa quando questi strumenti vengono dispiegati nella società. E quindi per comprendere meglio
questi strumenti, ho creato questo strumento chiamato Stable Bias Explorer, che permette di esplorare il pregiudizio dei
modelli di generazione di immagini attraverso la lente delle professioni. Quindi cerca di immaginare uno scienziato
nella tua mente. Non guardarmi. Cosa vedi? Più o meno la stessa cosa, giusto? Uomini con occhiali e camici da
laboratorio. E nessuno di loro mi assomiglia. E il fatto è che abbiamo esaminato tutti questi diversi modelli di
generazione di immagini e abbiamo trovato più o meno la stessa cosa: una significativa rappresentazione di bianchezza e
mascolinità attraverso tutte le professioni che abbiamo osservato, anche se rispetto al mondo reale, secondo l'Ufficio
di Statistica del Lavoro degli Stati Uniti. Questi modelli mostrano gli avvocati come uomini, e i CEO come uomini, quasi
il 100% delle volte anche se sappiamo tutti che non sono tutti bianchi e maschi. E purtroppo, il mio strumento non è
stato ancora utilizzato
per scrivere leggi. Ma di recente l'ho presentato a un evento delle Nazioni Unite sui pregiudizi di genere come esempio
di come possiamo creare strumenti per persone di ogni estrazione sociale, anche per coloro che non sanno programmare,
per impegnarsi e comprendere meglio l'AI perché utilizziamo le professioni, ma puoi utilizzare qualsiasi termine che ti
interessa. E man mano che questi modelli vengono dispiegati, vengono intessuti nel tessuto stesso delle nostre società,
nei nostri cellulari, nei nostri feed sui social media, persino nei nostri sistemi giudiziari e nelle nostre economie
c'è l'AI. Ed è davvero importante che l'AI rimanga accessibile in modo che sappiamo come funziona e quando non funziona.
E non c'è una soluzione unica per cose davvero complesse come i pregiudizi, il diritto d'autore o il cambiamento
climatico. Ma creando strumenti per misurare l'impatto dell'AI, possiamo iniziare a farci un'idea di quanto siano gravi
e iniziare ad affrontarli man mano che andiamo avanti. Iniziare a creare guardrail per proteggere la società e il
pianeta. E una volta che avremo queste informazioni, le aziende potranno usarle per dire: "OK, sceglieremo questo
modello perché è più sostenibile, questo modello perché rispetta il diritto d'autore". I legislatori che hanno davvero
bisogno di informazioni per scrivere leggi, possono utilizzare questi strumenti per sviluppare nuovi meccanismi di
regolamentazione o governance per l'AI man mano che viene dispiegata nella società. E gli utenti come te e me possiamo
utilizzare queste informazioni per scegliere modelli di AI di cui ci possiamo fidare, che non ci rappresentino in modo
scorretto e che non facciano un uso improprio dei nostri dati. Ma cosa ho risposto a quell'email che diceva che il mio
lavoro distruggerà l'umanità? Ho detto che concentrarsi sui rischi esistenziali futuri dell'AI è una distrazione dagli
impatti attuali, molto tangibili, e dal lavoro che dovremmo fare ora, o anche ieri, per ridurre questi impatti. Perché
sì, l'AI si sta muovendo velocemente, ma non è un fatto compiuto. Stiamo costruendo la strada mentre la percorriamo, e
possiamo decidere collettivamente in quale direzione vogliamo andare insieme.